#!/usr/bin/env python3
"""
Test script for comprehensive error handling system

This script tests various error scenarios to verify the error handling
system properly returns NaN for missing values and provides detailed
error reporting.
"""

import os
import sys
import json
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.data_extraction.error_handling_system import ErrorHandler, ErrorCategory
from src.data_extraction.excel_extraction_system import CellMappingParser

def test_error_handling():
    """Test various error handling scenarios"""
    
    print("B&R Capital Dashboard - Error Handling Test")
    print("=" * 50)
    
    # Initialize error handler
    error_handler = ErrorHandler()
    
    print("üß™ Testing Error Handling Scenarios...")
    print()
    
    # Test 1: Missing sheet
    print("1. Testing missing sheet error...")
    result1 = error_handler.handle_missing_sheet(
        "TEST_FIELD", "NonExistentSheet", ["Sheet1", "Sheet2", "Summary"]
    )
    print(f"   Result: {result1} (should be NaN)")
    print()
    
    # Test 2: Invalid cell address
    print("2. Testing invalid cell address...")
    result2 = error_handler.handle_invalid_cell_address(
        "TEST_FIELD", "Sheet1", "INVALID123", "Not a valid Excel address"
    )
    print(f"   Result: {result2} (should be NaN)")
    print()
    
    # Test 3: Formula errors
    print("3. Testing formula errors...")
    result3 = error_handler.handle_formula_error(
        "TEST_FIELD", "Sheet1", "A1", "#DIV/0!"
    )
    print(f"   Result: {result3} (should be NaN)")
    print()
    
    # Test 4: Process various cell values
    print("4. Testing cell value processing...")
    test_values = [
        (None, "null value"),
        ("", "empty string"),
        ("#REF!", "formula error"),
        ("N/A", "missing indicator"),
        (123.45, "valid number"),
        ("Valid Text", "valid text"),
        (0, "zero value")
    ]
    
    for value, description in test_values:
        result = error_handler.process_cell_value(
            value, "TEST_FIELD", "Sheet1", "A1"
        )
        print(f"   {description:15} '{value}' ‚Üí {result}")
    print()
    
    # Test 5: Error summary
    print("5. Testing error summary generation...")
    summary = error_handler.get_error_summary()
    
    print(f"   Total errors: {summary['total_errors']}")
    print(f"   Error categories: {len(summary['error_breakdown_by_category'])}")
    print(f"   Recommendations: {len(summary['recommendations'])}")
    
    if summary['recommendations']:
        print("   Top recommendations:")
        for rec in summary['recommendations'][:3]:
            print(f"     ‚Ä¢ {rec}")
    print()
    
    # Export error report
    print("6. Exporting error report...")
    report_file = project_root / "data" / "error_handling_test_report.json"
    report_file.parent.mkdir(parents=True, exist_ok=True)
    error_handler.export_error_report(str(report_file))
    print(f"   Error report saved: {report_file.name}")
    print()
    
    return True

def test_excel_extraction_with_errors():
    """Test Excel extraction with intentional errors"""
    
    print("üî¨ Testing Excel Extraction with Error Scenarios...")
    print("-" * 50)
    
    # File paths
    reference_file = Path("/home/mattb/B&R Programming (WSL)/B&R Capital Dashboard Project/Your Files/Underwriting Dashboard Project - Cell Value References.xlsx")
    test_excel_file = Path("/home/mattb/B&R Programming (WSL)/B&R Capital Dashboard Project/Your Files/Emparrado UW Model vCurrent.xlsb")
    
    if not reference_file.exists():
        print(f"‚ùå Reference file not found: {reference_file}")
        return False
        
    if not test_excel_file.exists():
        print(f"‚ùå Test Excel file not found: {test_excel_file}")
        return False
    
    print(f"‚úÖ Using reference file: {reference_file.name}")
    print(f"‚úÖ Using test file: {test_excel_file.name}")
    print()
    
    try:
        # Load mappings
        print("üìã Loading cell mappings...")
        parser = CellMappingParser(str(reference_file))
        mappings = parser.load_mappings()
        print(f"   Loaded {len(mappings)} mappings")
        print()
        
        # Create modified mappings with some intentional errors
        print("üéØ Creating test mappings with intentional errors...")
        test_mappings = {}
        
        # Add some valid mappings
        valid_count = 0
        for field_name, mapping in mappings.items():
            if valid_count < 10:  # Take first 10 valid mappings
                test_mappings[field_name] = mapping
                valid_count += 1
        
        # Add some mappings with errors
        from src.data_extraction.excel_extraction_system import CellMapping
        
        test_mappings["ERROR_MISSING_SHEET"] = CellMapping(
            "Test Category", "Missing Sheet Test", "NonExistentSheet", "A1", "ERROR_MISSING_SHEET"
        )
        test_mappings["ERROR_INVALID_CELL"] = CellMapping(
            "Test Category", "Invalid Cell Test", "Assumptions (Summary)", "INVALID123", "ERROR_INVALID_CELL"
        )
        test_mappings["ERROR_OUT_OF_BOUNDS"] = CellMapping(
            "Test Category", "Out of Bounds Test", "Assumptions (Summary)", "ZZ9999", "ERROR_OUT_OF_BOUNDS"
        )
        
        print(f"   Created {len(test_mappings)} test mappings ({valid_count} valid + 3 error cases)")
        print()
        
        # Extract with error handling
        from src.data_extraction.excel_extraction_system import ExcelDataExtractor
        
        print("‚ö° Running extraction with enhanced error handling...")
        extractor = ExcelDataExtractor(test_mappings)
        
        results = extractor.extract_from_file(str(test_excel_file))
        
        # Analyze results
        metadata = results.get('_extraction_metadata', {})
        error_summary = metadata.get('error_summary', {})
        
        print(f"üìä Extraction Results:")
        print(f"   Total fields: {metadata.get('total_fields', 0)}")
        print(f"   Successful: {metadata.get('successful', 0)}")
        print(f"   Failed: {metadata.get('failed', 0)}")
        print(f"   Duration: {metadata.get('duration_seconds', 0):.2f} seconds")
        print()
        
        if error_summary.get('total_errors', 0) > 0:
            print(f"üîç Error Analysis:")
            print(f"   Total errors: {error_summary['total_errors']}")
            
            categories = error_summary.get('error_breakdown_by_category', {})
            for category, info in categories.items():
                print(f"   {category}: {info['count']} errors ({info['percentage']}%)")
            
            print("   Recommendations:")
            for rec in error_summary.get('recommendations', []):
                print(f"     ‚Ä¢ {rec}")
        else:
            print("‚úÖ No errors detected!")
        
        print()
        
        # Save detailed results
        output_file = project_root / "data" / "error_handling_extraction_test.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"üíæ Detailed results saved: {output_file.name}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        print(f"Full error:\n{traceback.format_exc()}")
        return False

def main():
    """Run all error handling tests"""
    
    print("Starting comprehensive error handling tests...")
    print()
    
    success = True
    
    # Test 1: Basic error handling
    try:
        if not test_error_handling():
            success = False
    except Exception as e:
        print(f"‚ùå Basic error handling test failed: {e}")
        success = False
    
    print()
    
    # Test 2: Excel extraction with errors
    try:
        if not test_excel_extraction_with_errors():
            success = False
    except Exception as e:
        print(f"‚ùå Excel extraction error test failed: {e}")
        success = False
    
    print()
    print("=" * 60)
    
    if success:
        print("üéâ ALL ERROR HANDLING TESTS PASSED!")
        print()
        print("‚úÖ Key Features Verified:")
        print("  ‚Ä¢ NaN handling for missing values")
        print("  ‚Ä¢ Comprehensive error categorization")
        print("  ‚Ä¢ Detailed error reporting")
        print("  ‚Ä¢ Graceful degradation")
        print("  ‚Ä¢ Actionable recommendations")
        print()
        print("üöÄ Phase 2.4: Error handling with NaN - COMPLETE!")
    else:
        print("‚ùå Some tests failed. Check output above.")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())